{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from og_marl.vault_utils.download_vault import *\n",
    "from og_marl.vault_utils.analyse_vault import *\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_card(source,env,task,datacard_dict):\n",
    "    info_1_card = datacard_dict[source][env][task]\n",
    "\n",
    "    # make the relevant details table\n",
    "    table_components = [\"Environment name\",\"Version\",\"Agents\",\"Action type\",\"Observation size\",\"Reward type\"]\n",
    "\n",
    "    version_dict = {\n",
    "        \"og_marl\":{\n",
    "            \"smac_v1\": \"<a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL\",\n",
    "            \"smac_v2\": \"<a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL\",\n",
    "            \"mamujoco\": \"<a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.1'>V1.1</a>, Mujoco v210\",\n",
    "        },\n",
    "        \"cfcql\":{\n",
    "            \"smac_v1\": \"<a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL\",\n",
    "        },\n",
    "        \"alberdice\":{\n",
    "            \"rware\": \"<a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a>\",\n",
    "        },\n",
    "        \"omar\":{\n",
    "            \"mpe\":\"<a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a>\",\n",
    "            \"mamujoco\": \"<a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200\",\n",
    "        },\n",
    "        \"omiga\": {\n",
    "            \"smac_v1\":\"Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a>\",\n",
    "            \"mamujoco\": \"<a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    action_type_dict = {\n",
    "        \"smac_v1\": \"Discrete\",\n",
    "        \"smac_v2\": \"Discrete\",\n",
    "        \"mamujoco\": \"Continuous\",\n",
    "        \"mpe\": \"Discrete\",\n",
    "        \"rware\": \"Discrete\",\n",
    "    }\n",
    "\n",
    "    uid = list(info_1_card.keys())[0]\n",
    "\n",
    "    # action_size = 1\n",
    "    # if len(info_1_card[uid]['Structure']['actions'])>3:\n",
    "    #     action_size = info_1_card[uid]['Structure']['actions'][2:]\n",
    "\n",
    "    tab_1 = \"<table><tr>\"\n",
    "    for comp in table_components:\n",
    "        tab_1+=f\"<th>{comp}</th>\"\n",
    "    tab_1+=\"</tr><tr>\"\n",
    "    tab_1+=f\"<td>{info_1_card[uid]['Environment name']}</td>\"\n",
    "    tab_1+=f\"<td>{version_dict[source][env]}</td>\"\n",
    "    tab_1+=f\"<td>{info_1_card[uid]['Structure']['actions'][2]}</td>\"\n",
    "    tab_1+=f\"<td>{action_type_dict[env]}</td>\"\n",
    "    tab_1+=f\"<td>{info_1_card[uid]['Structure']['observations'][3:]}</td>\"\n",
    "    tab_1+=f\"<td>Dense</td>\"\n",
    "    tab_1+=\"</tr></table>\"\n",
    "\n",
    "    # Make the summary statistics table\n",
    "    table_components = [\"Min return\",\"Max return\",\"Transitions\",\"Trajectories\",\"Joint SACo\"]\n",
    "\n",
    "    html_table = \"<table><tr>\"\n",
    "    html_table+=\"<th>Uid</th>\"\n",
    "    html_table+=f\"<th>Episode return mean</th>\"\n",
    "    for comp in table_components:\n",
    "        html_table+=f\"<th>{comp}</th>\"\n",
    "\n",
    "    for uid in info_1_card.keys():\n",
    "        html_table+=\"</tr><tr>\"\n",
    "        html_table+=f\"<td>{uid}</td>\"\n",
    "        html_table+=str(fr\"<td>{info_1_card[uid]['Mean episode return']:.2f} \")+'&#177;'+ str(f\" {info_1_card[uid]['Standard deviation episode return']:.2f}</td>\")\n",
    "        for comp in table_components[:2]:\n",
    "            html_table+=f\"<td>{info_1_card[uid][comp]:.2f}</td>\"\n",
    "        for comp in table_components[2:-1]:\n",
    "            html_table+=f\"<td>{info_1_card[uid][comp]}</td>\"\n",
    "        for comp in table_components[-1:]:\n",
    "            html_table+=f\"<td>{info_1_card[uid][comp]:.2f}</td>\"\n",
    "    html_table+=\"</tr></table>\"\n",
    "\n",
    "    # image url\n",
    "    # image_url = info_1_card[uid]['Histogram download url']\n",
    "    # image_url = \"/home/louise/workspace/og-marl/docs/assets/vault_plots/og_marl/smac_v1/5m_vs_6m_histogram.pdf\"\n",
    "    image_url = f\"../assets/vault_plots/{source}/{env}/{task}_histogram.png\"\n",
    "    # print(image_url)\n",
    "    # image_url = 'blob:null/a6506be2-d5d5-4ba5-a39f-dfe2cdc618b6'\n",
    "\n",
    "    one_datacard = f\"<div class=\\\"card\\\"><img src=\\\"{image_url}\\\" alt=\\\"{info_1_card[uid]['Scenario name']}\\\" class=\\\"card-img\\\"><div class=\\\"card-content\\\"><h2>{info_1_card[uid]['Scenario name']} - <a href='{info_1_card[uid]['Download link']}'>Download</a></h2><h3>Metadata</h3><p>{tab_1}</p><h3>Generation procedure for each dataset</h3><p>{info_1_card[uid]['Generation procedure']}</p><h3>Summary statistics</h3><p>{html_table}</p></div></div>\"\n",
    "\n",
    "    # display(HTML(one_datacard))\n",
    "    print(one_datacard)\n",
    "    \n",
    "    return one_datacard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./datacard_info.json\") as current_info:\n",
    "    # get json string\n",
    "    datacard_str = json.load(current_info)\n",
    "\n",
    "    # convert to dictionary\n",
    "    datacard_dict = json.loads(datacard_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/2c_vs_64zg_histogram.png\" alt=\"2c_vs_64zg\" class=\"card-img\"><div class=\"card-content\"><h2>2c_vs_64zg - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/2c_vs_64zg.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>2</td><td>Discrete</td><td>[478]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>8.91 &#177; 1.01</td><td>2.53</td><td>10.00</td><td>10830</td><td>348</td><td>1.00</td></tr><tr><td>Medium</td><td>13.00 &#177; 1.39</td><td>10.01</td><td>15.00</td><td>37940</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>19.94 &#177; 1.26</td><td>15.18</td><td>21.61</td><td>59215</td><td>1001</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/6h_vs_8z_histogram.png\" alt=\"6h_vs_8z\" class=\"card-img\"><div class=\"card-content\"><h2>6h_vs_8z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/6h_vs_8z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>6</td><td>Discrete</td><td>[172]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>9.12 &#177; 0.81</td><td>4.80</td><td>9.99</td><td>24255</td><td>1001</td><td>1.00</td></tr><tr><td>Medium</td><td>11.97 &#177; 1.26</td><td>10.00</td><td>14.99</td><td>29511</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>17.84 &#177; 2.15</td><td>15.01</td><td>20.02</td><td>38040</td><td>1001</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>5</td><td>Discrete</td><td>[124]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>8.50 &#177; 1.19</td><td>1.81</td><td>9.89</td><td>22747</td><td>1001</td><td>0.96</td></tr><tr><td>Medium</td><td>11.03 &#177; 0.58</td><td>10.08</td><td>11.96</td><td>27717</td><td>1001</td><td>0.95</td></tr><tr><td>Good</td><td>20.00 &#177; 0.00</td><td>20.00</td><td>20.00</td><td>27734</td><td>1001</td><td>0.96</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/corridor_histogram.png\" alt=\"corridor\" class=\"card-img\"><div class=\"card-content\"><h2>corridor - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/corridor.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>6</td><td>Discrete</td><td>[346]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>4.93 &#177; 1.71</td><td>0.00</td><td>9.99</td><td>51268</td><td>1001</td><td>1.00</td></tr><tr><td>Medium</td><td>13.07 &#177; 1.27</td><td>10.02</td><td>14.99</td><td>126012</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>19.88 &#177; 1.01</td><td>15.01</td><td>20.49</td><td>100170</td><td>1001</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/6halfcheetah_histogram.png\" alt=\"6halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>6halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/6halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>6</td><td>Continuous</td><td>[23]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>655.76 &#177; 590.40</td><td>-198.77</td><td>2132.60</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium-Expert</td><td>2105.38 &#177; 1073.24</td><td>251.94</td><td>3866.09</td><td>2002000</td><td>2000</td><td>1.00</td></tr><tr><td>Medium</td><td>1425.66 &#177; 520.12</td><td>251.94</td><td>2113.52</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>2785.10 &#177; 1053.14</td><td>317.94</td><td>3866.09</td><td>1001000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/2ant_histogram.png\" alt=\"2ant\" class=\"card-img\"><div class=\"card-content\"><h2>2ant - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/2ant.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>2</td><td>Continuous</td><td>[113]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>1029.51 &#177; 141.27</td><td>895.37</td><td>1517.06</td><td>1751750</td><td>1750</td><td>0.66</td></tr><tr><td>Medium-Expert</td><td>1736.88 &#177; 319.64</td><td>840.77</td><td>2124.15</td><td>2002000</td><td>2000</td><td>1.00</td></tr><tr><td>Medium</td><td>1418.70 &#177; 37.04</td><td>840.77</td><td>1473.86</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>2055.07 &#177; 22.07</td><td>1994.03</td><td>2124.15</td><td>1001000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/3hopper_histogram.png\" alt=\"3hopper\" class=\"card-img\"><div class=\"card-content\"><h2>3hopper - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/3hopper.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>3</td><td>Continuous</td><td>[14]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>746.42 &#177; 671.89</td><td>70.76</td><td>2801.15</td><td>1314826</td><td>4160</td><td>1.00</td></tr><tr><td>Medium-Expert</td><td>1190.61 &#177; 973.40</td><td>95.27</td><td>3762.69</td><td>1919782</td><td>5481</td><td>1.00</td></tr><tr><td>Medium</td><td>723.57 &#177; 211.66</td><td>128.38</td><td>2776.49</td><td>919391</td><td>4000</td><td>1.00</td></tr><tr><td>Expert</td><td>2452.02 &#177; 1097.86</td><td>95.27</td><td>3762.69</td><td>1000391</td><td>1481</td><td>1.00</td></tr></table></p></div></div>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head>    <meta charset=\"UTF-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">    <title>Dataset Cards - OMIGA</title>    <link rel=\"stylesheet\" href=\"styles.css\">    <style>    * {    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\"container\">        <h1>Dataset Cards - OMIGA</h1>        <div class=\"card-grid\"><div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/2c_vs_64zg_histogram.png\" alt=\"2c_vs_64zg\" class=\"card-img\"><div class=\"card-content\"><h2>2c_vs_64zg - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/2c_vs_64zg.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>2</td><td>Discrete</td><td>[478]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>8.91 &#177; 1.01</td><td>2.53</td><td>10.00</td><td>10830</td><td>348</td><td>1.00</td></tr><tr><td>Medium</td><td>13.00 &#177; 1.39</td><td>10.01</td><td>15.00</td><td>37940</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>19.94 &#177; 1.26</td><td>15.18</td><td>21.61</td><td>59215</td><td>1001</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/6h_vs_8z_histogram.png\" alt=\"6h_vs_8z\" class=\"card-img\"><div class=\"card-content\"><h2>6h_vs_8z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/6h_vs_8z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>6</td><td>Discrete</td><td>[172]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>9.12 &#177; 0.81</td><td>4.80</td><td>9.99</td><td>24255</td><td>1001</td><td>1.00</td></tr><tr><td>Medium</td><td>11.97 &#177; 1.26</td><td>10.00</td><td>14.99</td><td>29511</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>17.84 &#177; 2.15</td><td>15.01</td><td>20.02</td><td>38040</td><td>1001</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>5</td><td>Discrete</td><td>[124]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>8.50 &#177; 1.19</td><td>1.81</td><td>9.89</td><td>22747</td><td>1001</td><td>0.96</td></tr><tr><td>Medium</td><td>11.03 &#177; 0.58</td><td>10.08</td><td>11.96</td><td>27717</td><td>1001</td><td>0.95</td></tr><tr><td>Good</td><td>20.00 &#177; 0.00</td><td>20.00</td><td>20.00</td><td>27734</td><td>1001</td><td>0.96</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/smac_v1/corridor_histogram.png\" alt=\"corridor\" class=\"card-img\"><div class=\"card-content\"><h2>corridor - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/smac_v1/corridor.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td>Modified version of SMAC v1, popularised by <a href='https://github.com/sanmuyang/multi-agent-PPO-on-SMAC?tab=readme-ov-file'>MAPPO </a></td><td>6</td><td>Discrete</td><td>[346]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>4.93 &#177; 1.71</td><td>0.00</td><td>9.99</td><td>51268</td><td>1001</td><td>1.00</td></tr><tr><td>Medium</td><td>13.07 &#177; 1.27</td><td>10.02</td><td>14.99</td><td>126012</td><td>1001</td><td>1.00</td></tr><tr><td>Good</td><td>19.88 &#177; 1.01</td><td>15.01</td><td>20.49</td><td>100170</td><td>1001</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/6halfcheetah_histogram.png\" alt=\"6halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>6halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/6halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>6</td><td>Continuous</td><td>[23]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>655.76 &#177; 590.40</td><td>-198.77</td><td>2132.60</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium-Expert</td><td>2105.38 &#177; 1073.24</td><td>251.94</td><td>3866.09</td><td>2002000</td><td>2000</td><td>1.00</td></tr><tr><td>Medium</td><td>1425.66 &#177; 520.12</td><td>251.94</td><td>2113.52</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>2785.10 &#177; 1053.14</td><td>317.94</td><td>3866.09</td><td>1001000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/2ant_histogram.png\" alt=\"2ant\" class=\"card-img\"><div class=\"card-content\"><h2>2ant - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/2ant.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>2</td><td>Continuous</td><td>[113]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>1029.51 &#177; 141.27</td><td>895.37</td><td>1517.06</td><td>1751750</td><td>1750</td><td>0.66</td></tr><tr><td>Medium-Expert</td><td>1736.88 &#177; 319.64</td><td>840.77</td><td>2124.15</td><td>2002000</td><td>2000</td><td>1.00</td></tr><tr><td>Medium</td><td>1418.70 &#177; 37.04</td><td>840.77</td><td>1473.86</td><td>1001000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>2055.07 &#177; 22.07</td><td>1994.03</td><td>2124.15</td><td>1001000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omiga/mamujoco/3hopper_histogram.png\" alt=\"3hopper\" class=\"card-img\"><div class=\"card-content\"><h2>3hopper - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omiga/mamujoco/3hopper.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>3</td><td>Continuous</td><td>[14]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omiga format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Medium-Replay</td><td>746.42 &#177; 671.89</td><td>70.76</td><td>2801.15</td><td>1314826</td><td>4160</td><td>1.00</td></tr><tr><td>Medium-Expert</td><td>1190.61 &#177; 973.40</td><td>95.27</td><td>3762.69</td><td>1919782</td><td>5481</td><td>1.00</td></tr><tr><td>Medium</td><td>723.57 &#177; 211.66</td><td>128.38</td><td>2776.49</td><td>919391</td><td>4000</td><td>1.00</td></tr><tr><td>Expert</td><td>2452.02 &#177; 1097.86</td><td>95.27</td><td>3762.69</td><td>1000391</td><td>1481</td><td>1.00</td></tr></table></p></div></div></div></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/3m_histogram.png\" alt=\"3m\" class=\"card-img\"><div class=\"card-content\"><h2>3m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/3m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>3</td><td>Discrete</td><td>[30]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>4.69 &#177; 2.14</td><td>0.00</td><td>20.00</td><td>997370</td><td>48779</td><td>0.81</td></tr><tr><td>Medium</td><td>9.96 &#177; 6.06</td><td>0.00</td><td>20.00</td><td>995313</td><td>41619</td><td>0.85</td></tr><tr><td>Good</td><td>16.49 &#177; 5.92</td><td>0.00</td><td>20.00</td><td>996366</td><td>43559</td><td>0.80</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/8m_histogram.png\" alt=\"8m\" class=\"card-img\"><div class=\"card-content\"><h2>8m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/8m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>8</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>5.28 &#177; 0.56</td><td>0.00</td><td>7.62</td><td>995144</td><td>20629</td><td>0.64</td></tr><tr><td>Medium</td><td>10.14 &#177; 3.34</td><td>0.00</td><td>20.00</td><td>996501</td><td>39208</td><td>0.96</td></tr><tr><td>Good</td><td>16.86 &#177; 4.33</td><td>0.19</td><td>20.00</td><td>997785</td><td>30638</td><td>0.86</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[55]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>7.45 &#177; 1.48</td><td>0.00</td><td>20.00</td><td>934505</td><td>45501</td><td>0.85</td></tr><tr><td>Medium</td><td>12.62 &#177; 5.06</td><td>0.00</td><td>20.00</td><td>996856</td><td>39284</td><td>0.87</td></tr><tr><td>Good</td><td>16.58 &#177; 4.69</td><td>0.00</td><td>20.00</td><td>996727</td><td>36311</td><td>0.84</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/2s3z_histogram.png\" alt=\"2s3z\" class=\"card-img\"><div class=\"card-content\"><h2>2s3z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/2s3z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>6.88 &#177; 2.06</td><td>0.00</td><td>13.61</td><td>996418</td><td>9942</td><td>0.96</td></tr><tr><td>Medium</td><td>12.57 &#177; 3.14</td><td>0.00</td><td>21.30</td><td>996256</td><td>18605</td><td>0.98</td></tr><tr><td>Good</td><td>18.32 &#177; 2.95</td><td>0.00</td><td>21.62</td><td>995829</td><td>18616</td><td>0.98</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/3s5z_vs_3s6z_histogram.png\" alt=\"3s5z_vs_3s6z\" class=\"card-img\"><div class=\"card-content\"><h2>3s5z_vs_3s6z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/3s5z_vs_3s6z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>8</td><td>Discrete</td><td>[136]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>5.90 &#177; 2.22</td><td>0.19</td><td>11.93</td><td>996474</td><td>17807</td><td>0.96</td></tr><tr><td>Medium</td><td>10.69 &#177; 1.49</td><td>0.00</td><td>17.67</td><td>996699</td><td>18866</td><td>0.97</td></tr><tr><td>Good</td><td>16.56 &#177; 3.72</td><td>6.30</td><td>24.46</td><td>996528</td><td>7315</td><td>0.97</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/terran_5_vs_5_histogram.png\" alt=\"terran_5_vs_5\" class=\"card-img\"><div class=\"card-content\"><h2>terran_5_vs_5 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/terran_5_vs_5.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[82]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>10.05 &#177; 5.84</td><td>0.00</td><td>36.34</td><td>898164</td><td>17958</td><td>1.00</td></tr><tr><td>Random</td><td>2.43 &#177; 1.73</td><td>0.00</td><td>16.18</td><td>1500000</td><td>37874</td><td>0.91</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/terran_10_vs_10_histogram.png\" alt=\"terran_10_vs_10\" class=\"card-img\"><div class=\"card-content\"><h2>terran_10_vs_10 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/terran_10_vs_10.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>10</td><td>Discrete</td><td>[162]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>6.32 &#177; 3.62</td><td>0.00</td><td>23.01</td><td>749850</td><td>13588</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/zerg_5_vs_5_histogram.png\" alt=\"zerg_5_vs_5\" class=\"card-img\"><div class=\"card-content\"><h2>zerg_5_vs_5 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/zerg_5_vs_5.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[82]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>7.34 &#177; 3.60</td><td>0.00</td><td>24.00</td><td>863281</td><td>23294</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/og_marl/mamujoco/2halfcheetah_histogram.png\" alt=\"2halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>2halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/mamujoco/2halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.1'>V1.1</a>, Mujoco v210</td><td>2</td><td>Continuous</td><td>[13]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A MATD3 system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. Gaussian noise with standard deviation of 0.2 was added to the action selection. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>400.45 &#177; 333.96</td><td>-191.49</td><td>905.03</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium</td><td>1485.00 &#177; 469.14</td><td>689.43</td><td>2332.17</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Good</td><td>6924.11 &#177; 1270.39</td><td>803.12</td><td>9132.25</td><td>1000000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head>    <meta charset=\"UTF-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">    <title>Dataset Cards - OG MARL</title>    <link rel=\"stylesheet\" href=\"styles.css\">    <style>    * {    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\"container\">        <h1>Dataset Cards - OG MARL</h1>        <div class=\"card-grid\"><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/3m_histogram.png\" alt=\"3m\" class=\"card-img\"><div class=\"card-content\"><h2>3m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/3m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>3</td><td>Discrete</td><td>[30]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>4.69 &#177; 2.14</td><td>0.00</td><td>20.00</td><td>997370</td><td>48779</td><td>0.81</td></tr><tr><td>Medium</td><td>9.96 &#177; 6.06</td><td>0.00</td><td>20.00</td><td>995313</td><td>41619</td><td>0.85</td></tr><tr><td>Good</td><td>16.49 &#177; 5.92</td><td>0.00</td><td>20.00</td><td>996366</td><td>43559</td><td>0.80</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/8m_histogram.png\" alt=\"8m\" class=\"card-img\"><div class=\"card-content\"><h2>8m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/8m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>8</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>5.28 &#177; 0.56</td><td>0.00</td><td>7.62</td><td>995144</td><td>20629</td><td>0.64</td></tr><tr><td>Medium</td><td>10.14 &#177; 3.34</td><td>0.00</td><td>20.00</td><td>996501</td><td>39208</td><td>0.96</td></tr><tr><td>Good</td><td>16.86 &#177; 4.33</td><td>0.19</td><td>20.00</td><td>997785</td><td>30638</td><td>0.86</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[55]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>7.45 &#177; 1.48</td><td>0.00</td><td>20.00</td><td>934505</td><td>45501</td><td>0.85</td></tr><tr><td>Medium</td><td>12.62 &#177; 5.06</td><td>0.00</td><td>20.00</td><td>996856</td><td>39284</td><td>0.87</td></tr><tr><td>Good</td><td>16.58 &#177; 4.69</td><td>0.00</td><td>20.00</td><td>996727</td><td>36311</td><td>0.84</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/2s3z_histogram.png\" alt=\"2s3z\" class=\"card-img\"><div class=\"card-content\"><h2>2s3z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/2s3z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>6.88 &#177; 2.06</td><td>0.00</td><td>13.61</td><td>996418</td><td>9942</td><td>0.96</td></tr><tr><td>Medium</td><td>12.57 &#177; 3.14</td><td>0.00</td><td>21.30</td><td>996256</td><td>18605</td><td>0.98</td></tr><tr><td>Good</td><td>18.32 &#177; 2.95</td><td>0.00</td><td>21.62</td><td>995829</td><td>18616</td><td>0.98</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v1/3s5z_vs_3s6z_histogram.png\" alt=\"3s5z_vs_3s6z\" class=\"card-img\"><div class=\"card-content\"><h2>3s5z_vs_3s6z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v1/3s5z_vs_3s6z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>8</td><td>Discrete</td><td>[136]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>5.90 &#177; 2.22</td><td>0.19</td><td>11.93</td><td>996474</td><td>17807</td><td>0.96</td></tr><tr><td>Medium</td><td>10.69 &#177; 1.49</td><td>0.00</td><td>17.67</td><td>996699</td><td>18866</td><td>0.97</td></tr><tr><td>Good</td><td>16.56 &#177; 3.72</td><td>6.30</td><td>24.46</td><td>996528</td><td>7315</td><td>0.97</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/terran_5_vs_5_histogram.png\" alt=\"terran_5_vs_5\" class=\"card-img\"><div class=\"card-content\"><h2>terran_5_vs_5 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/terran_5_vs_5.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[82]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>10.05 &#177; 5.84</td><td>0.00</td><td>36.34</td><td>898164</td><td>17958</td><td>1.00</td></tr><tr><td>Random</td><td>2.43 &#177; 1.73</td><td>0.00</td><td>16.18</td><td>1500000</td><td>37874</td><td>0.91</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/terran_10_vs_10_histogram.png\" alt=\"terran_10_vs_10\" class=\"card-img\"><div class=\"card-content\"><h2>terran_10_vs_10 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/terran_10_vs_10.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>10</td><td>Discrete</td><td>[162]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>6.32 &#177; 3.62</td><td>0.00</td><td>23.01</td><td>749850</td><td>13588</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/smac_v2/zerg_5_vs_5_histogram.png\" alt=\"zerg_5_vs_5\" class=\"card-img\"><div class=\"card-content\"><h2>zerg_5_vs_5 - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/smac_v2/zerg_5_vs_5.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v2)</td><td><a href='https://github.com/oxwhirl/smacv2'>SMAC V2</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[82]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Replay</td><td>7.34 &#177; 3.60</td><td>0.00</td><td>24.00</td><td>863281</td><td>23294</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/og_marl/mamujoco/2halfcheetah_histogram.png\" alt=\"2halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>2halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/core/mamujoco/2halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.1'>V1.1</a>, Mujoco v210</td><td>2</td><td>Continuous</td><td>[13]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>A MATD3 system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. Gaussian noise with standard deviation of 0.2 was added to the action selection. This procedure was repeated 4 times and the data was combined.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Poor</td><td>400.45 &#177; 333.96</td><td>-191.49</td><td>905.03</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium</td><td>1485.00 &#177; 469.14</td><td>689.43</td><td>2332.17</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Good</td><td>6924.11 &#177; 1270.39</td><td>803.12</td><td>9132.25</td><td>1000000</td><td>1000</td><td>1.00</td></tr></table></p></div></div></div></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/6h_vs_8z_histogram.png\" alt=\"6h_vs_8z\" class=\"card-img\"><div class=\"card-content\"><h2>6h_vs_8z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/6h_vs_8z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>6</td><td>Discrete</td><td>[78]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>17.81 &#177; 2.88</td><td>9.14</td><td>20.17</td><td>217723</td><td>5000</td><td>0.24</td></tr><tr><td>Medium-Replay</td><td>12.97 &#177; 2.22</td><td>0.81</td><td>20.03</td><td>182403</td><td>5000</td><td>1.00</td></tr><tr><td>Medium</td><td>16.63 &#177; 3.03</td><td>9.80</td><td>20.00</td><td>207008</td><td>5000</td><td>0.12</td></tr><tr><td>Expert</td><td>19.01 &#177; 2.11</td><td>9.14</td><td>20.17</td><td>228120</td><td>5000</td><td>0.12</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/3s_vs_5z_histogram.png\" alt=\"3s_vs_5z\" class=\"card-img\"><div class=\"card-content\"><h2>3s_vs_5z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/3s_vs_5z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>3</td><td>Discrete</td><td>[48]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>21.04 &#177; 2.51</td><td>5.58</td><td>29.00</td><td>888375</td><td>5000</td><td>0.23</td></tr><tr><td>Medium-Replay</td><td>18.85 &#177; 4.20</td><td>4.03</td><td>28.53</td><td>1082739</td><td>5000</td><td>0.99</td></tr><tr><td>Medium</td><td>20.86 &#177; 3.47</td><td>5.58</td><td>29.00</td><td>1174576</td><td>5000</td><td>0.11</td></tr><tr><td>Expert</td><td>21.19 &#177; 0.70</td><td>9.21</td><td>24.87</td><td>600520</td><td>5000</td><td>0.12</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[55]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>15.11 &#177; 5.11</td><td>6.38</td><td>20.00</td><td>131703</td><td>5000</td><td>0.22</td></tr><tr><td>Medium-Replay</td><td>9.02 &#177; 2.59</td><td>4.57</td><td>20.00</td><td>118405</td><td>5000</td><td>0.96</td></tr><tr><td>Medium</td><td>12.05 &#177; 4.36</td><td>6.38</td><td>20.00</td><td>135256</td><td>5000</td><td>0.10</td></tr><tr><td>Expert</td><td>18.17 &#177; 3.79</td><td>7.13</td><td>20.00</td><td>128536</td><td>5000</td><td>0.12</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/2s3z_histogram.png\" alt=\"2s3z\" class=\"card-img\"><div class=\"card-content\"><h2>2s3z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/2s3z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>16.39 &#177; 4.33</td><td>7.96</td><td>20.27</td><td>232528</td><td>5000</td><td>0.25</td></tr><tr><td>Medium-Replay</td><td>7.94 &#177; 3.41</td><td>2.00</td><td>20.12</td><td>100121</td><td>1976</td><td>1.00</td></tr><tr><td>Medium</td><td>12.76 &#177; 3.32</td><td>7.96</td><td>20.27</td><td>253992</td><td>5000</td><td>0.12</td></tr><tr><td>Expert</td><td>19.97 &#177; 0.37</td><td>13.90</td><td>20.08</td><td>211832</td><td>5000</td><td>0.12</td></tr></table></p></div></div>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head>    <meta charset=\"UTF-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">    <title>Dataset Cards - CFCQL</title>    <link rel=\"stylesheet\" href=\"styles.css\">    <style>    * {    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\"container\">        <h1>Dataset Cards - CFCQL</h1>        <div class=\"card-grid\"><div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/6h_vs_8z_histogram.png\" alt=\"6h_vs_8z\" class=\"card-img\"><div class=\"card-content\"><h2>6h_vs_8z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/6h_vs_8z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>6</td><td>Discrete</td><td>[78]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>17.81 &#177; 2.88</td><td>9.14</td><td>20.17</td><td>217723</td><td>5000</td><td>0.24</td></tr><tr><td>Medium-Replay</td><td>12.97 &#177; 2.22</td><td>0.81</td><td>20.03</td><td>182403</td><td>5000</td><td>1.00</td></tr><tr><td>Medium</td><td>16.63 &#177; 3.03</td><td>9.80</td><td>20.00</td><td>207008</td><td>5000</td><td>0.12</td></tr><tr><td>Expert</td><td>19.01 &#177; 2.11</td><td>9.14</td><td>20.17</td><td>228120</td><td>5000</td><td>0.12</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/3s_vs_5z_histogram.png\" alt=\"3s_vs_5z\" class=\"card-img\"><div class=\"card-content\"><h2>3s_vs_5z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/3s_vs_5z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>3</td><td>Discrete</td><td>[48]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>21.04 &#177; 2.51</td><td>5.58</td><td>29.00</td><td>888375</td><td>5000</td><td>0.23</td></tr><tr><td>Medium-Replay</td><td>18.85 &#177; 4.20</td><td>4.03</td><td>28.53</td><td>1082739</td><td>5000</td><td>0.99</td></tr><tr><td>Medium</td><td>20.86 &#177; 3.47</td><td>5.58</td><td>29.00</td><td>1174576</td><td>5000</td><td>0.11</td></tr><tr><td>Expert</td><td>21.19 &#177; 0.70</td><td>9.21</td><td>24.87</td><td>600520</td><td>5000</td><td>0.12</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/5m_vs_6m_histogram.png\" alt=\"5m_vs_6m\" class=\"card-img\"><div class=\"card-content\"><h2>5m_vs_6m - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/5m_vs_6m.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[55]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>15.11 &#177; 5.11</td><td>6.38</td><td>20.00</td><td>131703</td><td>5000</td><td>0.22</td></tr><tr><td>Medium-Replay</td><td>9.02 &#177; 2.59</td><td>4.57</td><td>20.00</td><td>118405</td><td>5000</td><td>0.96</td></tr><tr><td>Medium</td><td>12.05 &#177; 4.36</td><td>6.38</td><td>20.00</td><td>135256</td><td>5000</td><td>0.10</td></tr><tr><td>Expert</td><td>18.17 &#177; 3.79</td><td>7.13</td><td>20.00</td><td>128536</td><td>5000</td><td>0.12</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/cfcql/smac_v1/2s3z_histogram.png\" alt=\"2s3z\" class=\"card-img\"><div class=\"card-content\"><h2>2s3z - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/cfcql/smac_v1/2s3z.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>SMAC (v1)</td><td><a href='https://github.com/oxwhirl/smac/releases/tag/v1'>SMAC V1</a>, from OxWhiRL</td><td>5</td><td>Discrete</td><td>[80]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from cfcql format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Mixed</td><td>16.39 &#177; 4.33</td><td>7.96</td><td>20.27</td><td>232528</td><td>5000</td><td>0.25</td></tr><tr><td>Medium-Replay</td><td>7.94 &#177; 3.41</td><td>2.00</td><td>20.12</td><td>100121</td><td>1976</td><td>1.00</td></tr><tr><td>Medium</td><td>12.76 &#177; 3.32</td><td>7.96</td><td>20.27</td><td>253992</td><td>5000</td><td>0.12</td></tr><tr><td>Expert</td><td>19.97 &#177; 0.37</td><td>13.90</td><td>20.08</td><td>211832</td><td>5000</td><td>0.12</td></tr></table></p></div></div></div></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-2ag_histogram.png\" alt=\"small-2ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-2ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-2ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>2</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>7.12 &#177; 2.07</td><td>1.13</td><td>12.37</td><td>500000</td><td>1000</td><td>0.99</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-4ag_histogram.png\" alt=\"small-4ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-4ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-4ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>4</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>9.49 &#177; 0.84</td><td>3.93</td><td>12.08</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-6ag_histogram.png\" alt=\"small-6ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-6ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-6ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>6</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>10.76 &#177; 0.68</td><td>7.59</td><td>12.69</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-2ag_histogram.png\" alt=\"tiny-2ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-2ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-2ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>2</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>12.77 &#177; 1.56</td><td>1.97</td><td>16.81</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-4ag_histogram.png\" alt=\"tiny-4ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-4ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-4ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>4</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>15.67 &#177; 1.20</td><td>10.40</td><td>18.63</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-6ag_histogram.png\" alt=\"tiny-6ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-6ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-6ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>6</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>17.45 &#177; 1.01</td><td>11.88</td><td>19.97</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head>    <meta charset=\"UTF-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">    <title>Dataset Cards - Alberdice</title>    <link rel=\"stylesheet\" href=\"styles.css\">    <style>    * {    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\"container\">        <h1>Dataset Cards - Alberdice</h1>        <div class=\"card-grid\"><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-2ag_histogram.png\" alt=\"small-2ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-2ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-2ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>2</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>7.12 &#177; 2.07</td><td>1.13</td><td>12.37</td><td>500000</td><td>1000</td><td>0.99</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-4ag_histogram.png\" alt=\"small-4ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-4ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-4ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>4</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>9.49 &#177; 0.84</td><td>3.93</td><td>12.08</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/small-6ag_histogram.png\" alt=\"small-6ag\" class=\"card-img\"><div class=\"card-content\"><h2>small-6ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/small-6ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>6</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>10.76 &#177; 0.68</td><td>7.59</td><td>12.69</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-2ag_histogram.png\" alt=\"tiny-2ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-2ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-2ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>2</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>12.77 &#177; 1.56</td><td>1.97</td><td>16.81</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-4ag_histogram.png\" alt=\"tiny-4ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-4ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-4ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>4</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>15.67 &#177; 1.20</td><td>10.40</td><td>18.63</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/alberdice/rware/tiny-6ag_histogram.png\" alt=\"tiny-6ag\" class=\"card-img\"><div class=\"card-content\"><h2>tiny-6ag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/alberdice/rware/tiny-6ag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>RWARE</td><td><a href='https://github.com/dematsunaga/alberdice/tree/main/marl_env/marl/env'>Code included in Alberdice repository</a></td><td>6</td><td>Discrete</td><td>[71]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from alberdice format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Expert</td><td>17.45 &#177; 1.01</td><td>11.88</td><td>19.97</td><td>500000</td><td>1000</td><td>1.00</td></tr></table></p></div></div></div></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_spread_histogram.png\" alt=\"simple_spread\" class=\"card-img\"><div class=\"card-content\"><h2>simple_spread - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_spread.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>3</td><td>Discrete</td><td>[18]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>159.57 &#177; 60.46</td><td>-5.43</td><td>510.05</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>203.74 &#177; 80.49</td><td>35.69</td><td>582.09</td><td>97500</td><td>3900</td><td>1.00</td></tr><tr><td>Medium</td><td>273.39 &#177; 92.06</td><td>27.35</td><td>649.51</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>530.95 &#177; 71.41</td><td>54.96</td><td>743.89</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_tag_histogram.png\" alt=\"simple_tag\" class=\"card-img\"><div class=\"card-content\"><h2>simple_tag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_tag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>4</td><td>Discrete</td><td>[16]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-4.13 &#177; 10.81</td><td>-20.18</td><td>117.09</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>3.90 &#177; 20.28</td><td>-17.11</td><td>146.12</td><td>62500</td><td>2500</td><td>1.00</td></tr><tr><td>Medium</td><td>116.36 &#177; 58.86</td><td>-12.66</td><td>418.25</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>207.90 &#177; 77.51</td><td>-16.04</td><td>549.20</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_world_histogram.png\" alt=\"simple_world\" class=\"card-img\"><div class=\"card-content\"><h2>simple_world - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_world.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>4</td><td>Discrete</td><td>[24]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-6.83 &#177; 5.74</td><td>-17.81</td><td>54.41</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>1.23 &#177; 13.49</td><td>-17.56</td><td>112.90</td><td>80000</td><td>3200</td><td>1.00</td></tr><tr><td>Medium</td><td>65.86 &#177; 29.55</td><td>-9.15</td><td>198.82</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>85.21 &#177; 31.11</td><td>-11.55</td><td>238.70</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div>\n",
      "<div class=\"card\"><img src=\"../assets/vault_plots/omar/mamujoco/2halfcheetah_histogram.png\" alt=\"2halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>2halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mamujoco/2halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>2</td><td>Continuous</td><td>[6]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-282.89 &#177; 77.50</td><td>-516.90</td><td>-62.62</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>423.49 &#177; 655.68</td><td>-509.10</td><td>1993.00</td><td>460000</td><td>460</td><td>1.00</td></tr><tr><td>Medium</td><td>1568.87 &#177; 273.38</td><td>20.49</td><td>1904.56</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>3338.69 &#177; 252.58</td><td>852.45</td><td>3605.42</td><td>1000000</td><td>1000</td><td>1.00</td></tr></table></p></div></div>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head>    <meta charset=\"UTF-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">    <title>Dataset Cards - OMAR</title>    <link rel=\"stylesheet\" href=\"styles.css\">    <style>    * {    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\"container\">        <h1>Dataset Cards - OMAR</h1>        <div class=\"card-grid\"><div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_spread_histogram.png\" alt=\"simple_spread\" class=\"card-img\"><div class=\"card-content\"><h2>simple_spread - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_spread.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>3</td><td>Discrete</td><td>[18]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>159.57 &#177; 60.46</td><td>-5.43</td><td>510.05</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>203.74 &#177; 80.49</td><td>35.69</td><td>582.09</td><td>97500</td><td>3900</td><td>1.00</td></tr><tr><td>Medium</td><td>273.39 &#177; 92.06</td><td>27.35</td><td>649.51</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>530.95 &#177; 71.41</td><td>54.96</td><td>743.89</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_tag_histogram.png\" alt=\"simple_tag\" class=\"card-img\"><div class=\"card-content\"><h2>simple_tag - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_tag.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>4</td><td>Discrete</td><td>[16]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-4.13 &#177; 10.81</td><td>-20.18</td><td>117.09</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>3.90 &#177; 20.28</td><td>-17.11</td><td>146.12</td><td>62500</td><td>2500</td><td>1.00</td></tr><tr><td>Medium</td><td>116.36 &#177; 58.86</td><td>-12.66</td><td>418.25</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>207.90 &#177; 77.51</td><td>-16.04</td><td>549.20</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omar/mpe/simple_world_histogram.png\" alt=\"simple_world\" class=\"card-img\"><div class=\"card-content\"><h2>simple_world - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mpe/simple_world.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MPE</td><td><a href='https://github.com/ling-pan/OMAR/tree/master/multiagent-particle-envs'>Code included in OMAR repository<a></td><td>4</td><td>Discrete</td><td>[24]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-6.83 &#177; 5.74</td><td>-17.81</td><td>54.41</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>1.23 &#177; 13.49</td><td>-17.56</td><td>112.90</td><td>80000</td><td>3200</td><td>1.00</td></tr><tr><td>Medium</td><td>65.86 &#177; 29.55</td><td>-9.15</td><td>198.82</td><td>1000000</td><td>40000</td><td>1.00</td></tr><tr><td>Expert</td><td>85.21 &#177; 31.11</td><td>-11.55</td><td>238.70</td><td>1000000</td><td>40000</td><td>1.00</td></tr></table></p></div></div><div class=\"card\"><img src=\"../assets/vault_plots/omar/mamujoco/2halfcheetah_histogram.png\" alt=\"2halfcheetah\" class=\"card-img\"><div class=\"card-content\"><h2>2halfcheetah - <a href='https://huggingface.co/datasets/InstaDeepAI/og-marl/resolve/main/prior_work/omar/mamujoco/2halfcheetah.zip'>Download</a></h2><h3>Metadata</h3><p><table><tr><th>Environment name</th><th>Version</th><th>Agents</th><th>Action type</th><th>Observation size</th><th>Reward type</th></tr><tr><td>MAMuJoCo</td><td><a href='https://github.com/schroederdewitt/multiagent_mujoco/releases/tag/v1.0'>V1.0</a>, Mujoco v200</td><td>2</td><td>Continuous</td><td>[6]</td><td>Dense</td></tr></table></p><h3>Generation procedure for each dataset</h3><p>Converted from omar format to a Vault.</p><h3>Summary statistics</h3><p><table><tr><th>Uid</th><th>Episode return mean</th><th>Min return</th><th>Max return</th><th>Transitions</th><th>Trajectories</th><th>Joint SACo</th></tr><tr><td>Random</td><td>-282.89 &#177; 77.50</td><td>-516.90</td><td>-62.62</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Medium-Replay</td><td>423.49 &#177; 655.68</td><td>-509.10</td><td>1993.00</td><td>460000</td><td>460</td><td>1.00</td></tr><tr><td>Medium</td><td>1568.87 &#177; 273.38</td><td>20.49</td><td>1904.56</td><td>1000000</td><td>1000</td><td>1.00</td></tr><tr><td>Expert</td><td>3338.69 &#177; 252.58</td><td>852.45</td><td>3605.42</td><td>1000000</td><td>1000</td><td>1.00</td></tr></table></p></div></div></div></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# source = \"og_marl\"\n",
    "\n",
    "pretty_source_names = {\n",
    "    \"og_marl\": \"OG MARL\",\n",
    "    \"cfcql\": \"CFCQL\",\n",
    "    \"omiga\": \"OMIGA\",\n",
    "    \"omar\":\"OMAR\",\n",
    "    \"alberdice\":\"Alberdice\",\n",
    "}\n",
    "\n",
    "with open(\"./datacard_info.json\") as current_info:\n",
    "    # get json string\n",
    "    datacard_str = json.load(current_info)\n",
    "\n",
    "    # convert to dictionary\n",
    "    datacard_dict = json.loads(datacard_str)\n",
    "\n",
    "for source in datacard_dict.keys():\n",
    "    start = f\"<!DOCTYPE html><html lang=\\\"en\\\"><head>    <meta charset=\\\"UTF-8\\\">    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">    <title>Dataset Cards - {pretty_source_names[source]}</title>    <link rel=\\\"stylesheet\\\" href=\\\"styles.css\\\">    <style>    * \"\n",
    "    start += \"{    margin: 0;    padding: 0;    box-sizing: border-box;} body {    font-family: Arial, sans-serif;    background-color: #f4f4f4;} .container {    max-width: 1200px;    margin: 0 auto;    padding: 20px;} h1 {    text-align: center;    margin-bottom: 20px;} .card-grid {    display: grid;    grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));    gap: 20px;} .card {    background-color: white;    border-radius: 8px;    box-shadow: 0 2px 5px rgba(0,0,0,0.1);    overflow: hidden;} .card-img {    width: 100%;    height: auto;} .card-content {    padding: 15px;} .card-content h2 {    font-size: 1.5em;    margin-bottom: 10px;} .card-content p {    color: #555;}    </style></head><body>    <div class=\\\"container\\\">        \"\n",
    "    start+= f\"<h1>Dataset Cards - {pretty_source_names[source]}</h1>        <div class=\\\"card-grid\\\">\"\n",
    "\n",
    "    for env in datacard_dict[source].keys():\n",
    "        if not env==\"gymnasium_mamujoco\":\n",
    "            for task in datacard_dict[source][env].keys():\n",
    "                start += make_one_card(source,env,task,datacard_dict)\n",
    "\n",
    "    start += \"</div></div></body></html>\"\n",
    "\n",
    "    display(HTML(start))\n",
    "\n",
    "    with open(f'{source}.md', 'w') as f:\n",
    "        f.write(start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "og-marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
