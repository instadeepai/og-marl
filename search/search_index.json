{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Offline Multi-Agent Reinforcement Learning Datasets and Baselines</p> <p> </p> <p> </p>"},{"location":"#going-off-the-grid","title":"Going Off-the-Grid! \ud83e\udd16 \u26a1 \ud83d\udd0c \ud83d\udd0b","text":"<p>Offline MARL holds great promise for real-world applications by utilising static datasets to build decentralised controllers of complex multi-agent systems. However, currently offline MARL lacks a standardised benchmark for measuring meaningful research progress. Off-the-Grid MARL (OG-MARL) fills this gap by providing a diverse suite of datasets with baselines on popular MARL benchmark environments in one place, with a unified API and an easy-to-use set of tools.</p> <p>OG-MARL forms part of the InstaDeep MARL ecosystem, developed jointly with the open-source community. To join us in these efforts, reach out, raise issues or just \ud83c\udf1f to stay up to date with the latest developments! \ud83d\udce2 You can contribute to the conversation around OG-MARL in the Discussion tab. Please don't hesitate to leave a comment. We will be happy to reply.</p> <p>\ud83d\udcf9 NeurIPS video: https://neurips.cc/virtual/2024/poster/97812</p> <p>\ud83d\udce2 We recently moved our datasets to Hugging Face. This means that previous download links for the datasets may no longer work. Datasets can now be downloaded directly from Hugging Face.</p>"},{"location":"#quickstart","title":"Quickstart \ud83c\udfce\ufe0f","text":"<p>Clone this repository.</p> <p><code>git clone https://github.com/instadeepai/og-marl.git</code></p> <p>Install <code>og-marl</code> and its requirements. We tested <code>og-marl</code> with Python 3.10 and Ubuntu 20.04. Consider using a <code>conda</code> virtual environment.</p> <p><code>pip install -e .[tf2_baselines]</code></p> <p>Download environment files. We will use SMACv1 in this example. MAMuJoCo installation instructions are included near the bottom of the README.</p> <p><code>bash install_environments/smacv1.sh</code></p> <p>Download environment requirements.</p> <p><code>pip install -r install_environments/requirements/smacv1.txt</code></p> <p>Train an offline system. In this example we will run Independent Q-Learning with Conservative Q-Learning (iql+cql). The script will automatically download the neccessary dataset if it is not found locally.</p> <p><code>python og_marl/baselines/tf2_systems/offline/iql_cql.py task.source=og_marl task.env=smac_v1 task.scenario=3m task.dataset=Good</code></p> <p>You can find all offline systems at <code>og_marl/tf2_systems/offline/</code> and they can be run similarly. Be careful, some systems only work on discrete action space environments and vice versa for continuous action space environments. The config files for systems are found at <code>og_marl/tf2_systems/offline/configs/</code>. We use hydra for our config management. Config defaults can be overwritten as command line arguments like above.</p>"},{"location":"#dataset-api","title":"Dataset API \ud83d\udd0c","text":"<p>To quickly start working with a dataset you do not even need to install <code>og-marl</code>.  Simply install Flashbax and download a dataset from Hugging Face. </p> <p><code>pip install flashbax</code></p> <p>Then you should be able to do something like this.</p> <pre><code>from flashbax.vault import Vault\nimport jax\nimport numpy as np\n\nvault = Vault(\"og_marl/smac_v1/2s3z.vlt\", vault_uid=\"Good\")\n\nexperience = vault.read().experience\n\nnumpy_experience = jax.tree.map(lambda x: np.array(x), experience)\n</code></pre> <p>We also provide a simple demonstrative notebook of how to use OG-MARL's dataset API here:</p> <p></p>"},{"location":"#datasets","title":"Datasets \ud83c\udfa5","text":"<p>We have generated datasets on a diverse set of popular MARL environments. A list of currently supported environments is included in the table below. It is well known from the single-agent offline RL literature that the quality of experience in offline datasets can play a large role in the final performance of offline RL algorithms. Therefore in OG-MARL, for each environment and scenario, we include a range of dataset distributions including <code>Good</code>, <code>Medium</code>, <code>Poor</code> and <code>Replay</code> datasets in order to benchmark offline MARL algorithms on a range of different dataset qualities. For more information on why we chose to include each environment and its task properties, please read our accompanying paper.</p> <p></p> <p>Our datasets are now hosted on Hugging Face for improved accessibility for the community: https://huggingface.co/datasets/InstaDeepAI/og-marl</p> <p>\u26a0\ufe0f Some datasets have yet to be converted to the new dataset format (Vault). For available datasets, please refer to <code>og_marl/vault_utils/download_vault.py</code> or the Hugging Face datasets repository.</p> <p></p>"},{"location":"#environments-and-scenarios-in-og-marl","title":"Environments and Scenarios in OG-MARL \ud83d\uddfa\ufe0f","text":"Environment Scenario Agents Act Obs Reward Types Repo \ud83d\udd2bSMAC v1 3m  8m  2s3z  5m_vs_6m  27m_vs_30m  3s5z_vs_3s6z  2c_vs_64zg 3  8  5  5  27  8  2 Discrete Vector Dense Homog  Homog  Heterog  Homog  Homog  Heterog  Homog source \ud83d\udca3SMAC v2 terran_5_vs_5  zerg_5_vs_5  terran_10_vs_10 5  5  10 Discrete Vector Dense Heterog source \ud83d\ude85Flatland 3 Trains   5 Trains 3  5 Discrete Vector Sparse Homog source \ud83d\udc1cMAMuJoCo 2x3 HalfCheetah  2x4 Ant  4x2 Ant 2  2  4 Cont. Vector Dense Heterog  Homog  Homog source \ud83d\udc3bPettingZoo Pursuit   Co-op Pong 8  2 Discrete  Discrete Pixels  Pixels Dense Homog  Heterog source"},{"location":"#datasets-from-prior-works","title":"Datasets from Prior Works \ud83e\udd47","text":"<p>We recently converted several datasets from prior works to Vaults and benchmarked our baseline algorithms on them. For more information, see our technical report on ArXiv.</p> Paper Environment Scenario Source Pan et al. (2022) \ud83d\udc1cMAMuJoCo 2x3 HalfCheetah source Pan et al. (2022) \ud83d\udd34MPE simple_spread source Shao et al. (2023) \ud83d\udd2bSMAC v1 5m_vs_6m  2s3z  3s_vs_5z  6h_vs_8z source Wang et al. (2023) \ud83d\udd2bSMAC v1 5m_vs_6m  6h_vs_8z  2c_vs_64zg  corridor source Wang et al. (2023) \ud83d\udc1cMAMuJoCo 6x1 HalfCheetah  3x1 Hopper  2x4 Ant source"},{"location":"#installing-mamujoco","title":"Installing MAMuJoCo \ud83d\udc06","text":"<p>The OG-MARL datasets use the latest version of MuJoCo (210). While the OMIGA and OMAR datasets use an older version (200). They each have different instalation instructions and should be installed in seperate virtual environments.</p>"},{"location":"#mamujoco-210","title":"MAMuJoCo 210","text":"<p><code>bash install_environments/mujoco210.sh</code></p> <p><code>pip install -r install_environments/requirements/mujoco.txt</code></p> <p><code>pip install -r install_environments/requirements/mamujoco210.txt</code></p>"},{"location":"#mamujoco-200","title":"MAMuJoCo 200","text":"<p><code>bash install_environments/mujoco200.sh</code></p> <p><code>pip install -r install_environments/requirements/mujoco.txt</code></p> <p><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco200/bin</code></p> <p><code>pip install -r install_environments/requirements/mamujoco200.txt</code></p>"},{"location":"#baselines-now-in-pytorch","title":"Baselines Now in PyTorch","text":"<p>The original OG-MARL baselines were implmented in TF2. We have now started to include PyTorch implementations. So far we have PyTorch varients of <code>iql_cql.py</code> and <code>iddpg_bc.py</code>.</p> <p></p>"},{"location":"#see-also","title":"See Also \ud83d\udd0e","text":"<p>InstaDeep's MARL ecosystem in JAX. In particular, we suggest users check out the following sister repositories:</p> <ul> <li>\ud83e\udd81 Mava: a research-friendly codebase for distributed MARL in JAX.</li> <li>\ud83c\udf34 Jumanji: a diverse suite of scalable reinforcement learning environments in JAX.</li> <li>\ud83d\ude0e Matrax: a collection of matrix games in JAX.</li> <li>\ud83d\udd26 Flashbax: accelerated replay buffers in JAX.</li> <li>\ud83d\udcc8 MARL-eval: standardised experiment data aggregation and visualisation for MARL.</li> </ul> <p>Related. Other libraries related to accelerated MARL in JAX.</p> <ul> <li>\ud83e\udd8a JaxMARL: accelerated MARL environments with baselines in JAX.</li> <li>\u265f\ufe0f  Pgx: JAX implementations of classic board games, such as Chess, Go and Shogi.</li> <li>\ud83d\udd3c Minimax: JAX implementations of autocurricula baselines for RL.</li> </ul>"},{"location":"#citing-og-marl","title":"Citing OG-MARL","text":""},{"location":"#if-you-use-og-marl-datasets-in-your-work-please-cite-the-library-using","title":"If you use OG-MARL Datasets in your work, please cite the library using:","text":"<pre><code>@inproceedings{formanek2023ogmarl,\n    author = {Formanek, Claude and Jeewa, Asad and Shock, Jonathan and Pretorius, Arnu},\n    title = {Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning},\n    year = {2023},\n    publisher = {AAMAS},\n    booktitle = {Extended Abstract at the 2023 International Conference on Autonomous Agents and Multiagent Systems},\n}\n</code></pre>"},{"location":"#if-you-use-og-marl-baselines-in-your-work-please-cite-the-library-using","title":"If you use OG-MARL Baselines in your work, please cite the library using:","text":"<pre><code>@inproceedings{NEURIPS2024_fc6247c3,\n author = {Formanek, Juan and Tilbury, Callum R. and Beyers, Louise and Shock, Jonathan and Pretorius, Arnu},\n booktitle = {Advances in Neural Information Processing Systems},\n pages = {139650--139672},\n publisher = {Curran Associates, Inc.},\n title = {Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation},\n volume = {37},\n year = {2024}\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements \ud83d\ude4f","text":"<p>The development of this library was supported with Cloud TPUs from Google's TPU Research Cloud (TRC) \ud83c\udf24.</p>"},{"location":"api/","title":"API Reference (Coming Soon)","text":""},{"location":"datasets/","title":"Datasets","text":"Website Page Datasets SMAC V1 <ul> <li>3m</li> <li>8m</li> <li>5m_vs_6m</li> <li>2s3z</li> <li>3s5z_vs_3s6z</li> <li>2c_vs_64zg</li> <li>27m_vs_30m</li> </ul> SMAC V2 <ul> <li>terran_5_vs_5</li> <li>zerg_5_vs_5</li> <li>terran_10_vs_10</li> </ul> Flatland <ul> <li>3_trains</li> <li>5_trains</li> </ul> PettingZoo <ul> <li>pursuit</li> <li>pistonball</li> <li>coop_pong</li> <li>kaz</li> </ul> MaMuJoCo <ul> <li>2_halfcheetah</li> <li>2_ant</li> <li>4_ant</li> </ul> Voltage Control <ul> <li>case33_3min_final</li> </ul> City Learn <ul> <li>2022_all_phases</li> </ul> MPE <ul> <li>simple_adversary</li> </ul>"},{"location":"updates/","title":"Updates","text":""},{"location":"updates/#updates-06122023","title":"Updates [06/12/2023] \ud83d\udcf0","text":"<p>OG-MARL is a research tool that is under active development and therefore evolving quickly. We have several very exciting new features on the roadmap but sometimes when we introduce a new feature we may abruptly change how things work in OG-MARL. But in the interest of moving quickly, we believe this is an acceptable trade-off and ask our users to kindly be aware of this.</p> <p>The following is a list of the latest updates to OG-MARL:</p> <p>\u2705 We have removed several cumbersome dependencies from OG-MARL, including <code>reverb</code> and <code>launchpad</code>. This means that its significantly easier to install and use OG-MARL.</p> <p>\u2705 We added functionality to pre-load the TF Record datasets into a Cpprb replay buffer. This speeds up the time to sample the replay buffer by several orders of magnitude.</p> <p>\u2705 We have implemented our first set of JAX-based systems in OG-MARL. Our JAX systems use Flashbax as the replay buffer backend. Flashbax buffers are completely jit-able, which means that our JAX systems have fully integrated and jitted training and data sampling.</p> <p>\u2705 We have integrated MARL-eval into OG-MARL to standardise and simplify the reporting of experimental results.</p>"},{"location":"updates/#need-for-speed","title":"Need for Speed \ud83c\udfce\ufe0f","text":"<p>We have made our TF2 systems compatible with jit compilation. This combined with our new <code>cpprb</code> replay buffers have made our systems significantly faster. Furthermore, our JAX systems with tightly integrated replay sampling and training using Flashbax are even faster.</p> <p>Speed Comparison: for each setup, we trained MAICQ on the 8m Good dataset for 10k training steps and evaluated every 1k training steps for 4 episodes using a batch size of 256.</p> <p>Performance Comparison: In order to make sure performance between the TF2 system and the JAX system is the same, we trained both variants on each of the three datasets for 8m (Good, Medium and Poor). We then normalised the scores and aggregated the results using MARL-eval. The sample efficiency curves and the performance profiles are given below.</p>"},{"location":"videos/","title":"Videos","text":"Website Page Videos about OG-MARL  NeurIPS '24 Poster Presentation   AARG Reading Group Presentation"},{"location":"baselines/flatland/","title":"Flatland Baseline Results","text":""},{"location":"baselines/mamujoco/","title":"MAMuJoCo Baseline Results","text":""},{"location":"baselines/pettingzoo/","title":"PettingZoo Baseline Results","text":""},{"location":"baselines/smac_v1/","title":"SMAC v1 Baseline Results","text":""},{"location":"baselines/smac_v2/","title":"SMAC v2 Baseline Results","text":""},{"location":"dataset_cards/","title":"","text":"Overview All Datasets  <pre><code>{\"og_marl\": {\n        \"smac_v1\": {\n            \"3m\": [\"Good\", \"Medium\", \"Poor\"],\n            \"8m\": [\"Good\", \"Medium\", \"Poor\"],\n            \"5m_vs_6m\": [\"Good\", \"Medium\", \"Poor\"],\n            \"2s3z\": [\"Good\", \"Medium\", \"Poor\"],\n            \"3s5z_vs_3s6z\": [\"Good\", \"Medium\", \"Poor\"],\n        },\n        \"smac_v2\": {\n            \"terran_5_vs_5\": [\"Replay\"],\n            \"terran_10_vs_10\": [\"Replay\"],\n            \"zerg_5_vs_5\": [\"Replay\"],\n        },\n        \"mamujoco\": {\n            \"2halfcheetah\": [\"Good\", \"Medium\", \"Poor\"]\n        },\n        \"gymnasium_mamujoco\": {\n            \"2ant\": [\"Replay\"],\n            \"2halfcheetah\": [\"Replay\"],\n            \"2walker\": [\"Replay\"],\n            \"3hopper\": [\"Replay\"],\n            \"4ant\": [\"Replay\"],\n            \"6halfcheetah\": [\"Replay\"],\n        },\n    },\n    \"cfcql\": {\n        \"smac_v1\": {\n            \"6h_vs_8z\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Mixed\"],\n            \"3s_vs_5z\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Mixed\"]\n            \"5m_vs_6m\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Mixed\"]\n            \"2s3z\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Mixed\"]\n        },\n    },\n    \"alberdice\": {\n        \"rware\": {\n            \"small-2ag\": [\"Expert\"],\n            \"small-4ag\": [\"Expert\"],\n            \"small-6ag\": [\"Expert\"],\n            \"tiny-2ag\": [\"Expert\"],\n            \"tiny-4ag\": [\"Expert\"],\n            \"tiny-6ag\": [\"Expert\"],\n        },\n    },\n    \"omar\": {\n        \"mpe\": {\n            \"simple_spread\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Random\"]\n            \"simple_tag\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Random\"]\n            \"simple_world\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Random\"]\n        },\n        \"mamujoco\": {\n            \"2halfcheetah\": [\"Expert\", \"Medium\", \"Medium-Replay\", \"Random\"]\n        },\n    },\n    \"omiga\": {\n        \"smac_v1\": {\n            \"2c_vs_64zg\": [\"Good\", \"Medium\", \"Poor\"],\n            \"6h_vs_8z\": [\"Good\", \"Medium\", \"Poor\"],\n            \"5m_vs_6m\": [\"Good\", \"Medium\", \"Poor\"],\n            \"corridor\": [\"Good\", \"Medium\", \"Poor\"],\n        },\n        \"mamujoco\": {\n            \"6halfcheetah\": [\"Expert\", \"Medium\", \"Medium-Expert\", \"Medium-Replay\"],\n            \"2ant\": [\"Expert\", \"Medium\", \"Medium-Expert\", \"Medium-Replay\"],\n            \"3hopper\": [\"Expert\", \"Medium\", \"Medium-Expert\", \"Medium-Replay\"],\n        },\n    },\n}\n</code></pre>"},{"location":"dataset_cards/alberdice/","title":"Alberdice","text":"Dataset Cards - Alberdice Dataset Cards - Alberdice small-2ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository2Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert7.12 \u00b1 2.071.1312.3750000010000.99</p>small-4ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository4Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert9.49 \u00b1 0.843.9312.0850000010001.00</p>small-6ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository6Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert10.76 \u00b1 0.687.5912.6950000010001.00</p>tiny-2ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository2Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert12.77 \u00b1 1.561.9716.8150000010001.00</p>tiny-4ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository4Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert15.67 \u00b1 1.2010.4018.6350000010001.00</p>tiny-6ag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeRWARECode included in Alberdice repository6Discrete[71]Dense</p>Generation procedure for each dataset<p>Converted from alberdice format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoExpert17.45 \u00b1 1.0111.8819.9750000010001.00</p>"},{"location":"dataset_cards/cfcql/","title":"CFCQL","text":"Dataset Cards - CFCQL Dataset Cards - CFCQL 6h_vs_8z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL6Discrete[78]Dense</p>Generation procedure for each dataset<p>Converted from cfcql format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMixed17.81 \u00b1 2.889.1420.1721772350000.24Medium-Replay12.97 \u00b1 2.220.8120.0318240350001.00Medium16.63 \u00b1 3.039.8020.0020700850000.12Expert19.01 \u00b1 2.119.1420.1722812050000.12</p>3s_vs_5z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL3Discrete[48]Dense</p>Generation procedure for each dataset<p>Converted from cfcql format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMixed21.04 \u00b1 2.515.5829.0088837550000.23Medium-Replay18.85 \u00b1 4.204.0328.53108273950000.99Medium20.86 \u00b1 3.475.5829.00117457650000.11Expert21.19 \u00b1 0.709.2124.8760052050000.12</p>5m_vs_6m - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL5Discrete[55]Dense</p>Generation procedure for each dataset<p>Converted from cfcql format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMixed15.11 \u00b1 5.116.3820.0013170350000.22Medium-Replay9.02 \u00b1 2.594.5720.0011840550000.96Medium12.05 \u00b1 4.366.3820.0013525650000.10Expert18.17 \u00b1 3.797.1320.0012853650000.12</p>2s3z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL5Discrete[80]Dense</p>Generation procedure for each dataset<p>Converted from cfcql format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMixed16.39 \u00b1 4.337.9620.2723252850000.25Medium-Replay7.94 \u00b1 3.412.0020.1210012119761.00Medium12.76 \u00b1 3.327.9620.2725399250000.12Expert19.97 \u00b1 0.3713.9020.0821183250000.12</p>"},{"location":"dataset_cards/og_marl/","title":"OG MARL","text":"Dataset Cards - OG MARL Dataset Cards - OG MARL 3m - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL3Discrete[30]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor4.69 \u00b1 2.140.0020.00997370487790.81Medium9.96 \u00b1 6.060.0020.00995313416190.85Good16.49 \u00b1 5.920.0020.00996366435590.80</p>8m - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL8Discrete[80]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor5.28 \u00b1 0.560.007.62995144206290.64Medium10.14 \u00b1 3.340.0020.00996501392080.96Good16.86 \u00b1 4.330.1920.00997785306380.86</p>5m_vs_6m - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL5Discrete[55]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor7.45 \u00b1 1.480.0020.00934505455010.85Medium12.62 \u00b1 5.060.0020.00996856392840.87Good16.58 \u00b1 4.690.0020.00996727363110.84</p>2s3z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL5Discrete[80]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor6.88 \u00b1 2.060.0013.6199641899420.96Medium12.57 \u00b1 3.140.0021.30996256186050.98Good18.32 \u00b1 2.950.0021.62995829186160.98</p>3s5z_vs_3s6z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)SMAC V1, from OxWhiRL8Discrete[136]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor5.90 \u00b1 2.220.1911.93996474178070.96Medium10.69 \u00b1 1.490.0017.67996699188660.97Good16.56 \u00b1 3.726.3024.4699652873150.97</p>terran_5_vs_5 - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v2)SMAC V2, from OxWhiRL5Discrete[82]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. An epsilon greedy policy with eps=0.05 was used. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoReplay10.05 \u00b1 5.840.0036.34898164179581.00Random2.43 \u00b1 1.730.0016.181500000378740.91</p>terran_10_vs_10 - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v2)SMAC V2, from OxWhiRL10Discrete[162]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoReplay6.32 \u00b1 3.620.0023.01749850135881.00</p>zerg_5_vs_5 - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v2)SMAC V2, from OxWhiRL5Discrete[82]Dense</p>Generation procedure for each dataset<p>A QMIX system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 1m transitions. An epsilon greedy policy with eps=0.05 was used.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoReplay7.34 \u00b1 3.600.0024.00863281232941.00</p>2halfcheetah - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMAMuJoCoV1.1, Mujoco v2102Continuous[13]Dense</p>Generation procedure for each dataset<p>A MATD3 system was trained to target level of performance. The learnt policy was then rolled out to collect approximately 250k transitions. Gaussian noise with standard deviation of 0.2 was added to the action selection. This procedure was repeated 4 times and the data was combined.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor400.45 \u00b1 333.96-191.49905.03100000010001.00Medium1485.00 \u00b1 469.14689.432332.17100000010001.00Good6924.11 \u00b1 1270.39803.129132.25100000010001.00</p>"},{"location":"dataset_cards/omar/","title":"OMAR","text":"Dataset Cards - OMAR Dataset Cards - OMAR simple_spread - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMPECode included in OMAR repository3Discrete[18]DenseGeneration procedure for each dataset<p>Converted from omar format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoRandom159.57 \u00b1 60.46-5.43510.051000000400001.00Medium-Replay203.74 \u00b1 80.4935.69582.099750039001.00Medium273.39 \u00b1 92.0627.35649.511000000400001.00Expert530.95 \u00b1 71.4154.96743.891000000400001.00</p>simple_tag - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMPECode included in OMAR repository4Discrete[16]DenseGeneration procedure for each dataset<p>Converted from omar format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoRandom-4.13 \u00b1 10.81-20.18117.091000000400001.00Medium-Replay3.90 \u00b1 20.28-17.11146.126250025001.00Medium116.36 \u00b1 58.86-12.66418.251000000400001.00Expert207.90 \u00b1 77.51-16.04549.201000000400001.00</p>simple_world - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMPECode included in OMAR repository4Discrete[24]DenseGeneration procedure for each dataset<p>Converted from omar format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoRandom-6.83 \u00b1 5.74-17.8154.411000000400001.00Medium-Replay1.23 \u00b1 13.49-17.56112.908000032001.00Medium65.86 \u00b1 29.55-9.15198.821000000400001.00Expert85.21 \u00b1 31.11-11.55238.701000000400001.00</p>2halfcheetah - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMAMuJoCoV1.0, Mujoco v2002Continuous[6]Dense</p>Generation procedure for each dataset<p>Converted from omar format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoRandom-282.89 \u00b1 77.50-516.90-62.62100000010001.00Medium-Replay423.49 \u00b1 655.68-509.101993.004600004601.00Medium1568.87 \u00b1 273.3820.491904.56100000010001.00Expert3338.69 \u00b1 252.58852.453605.42100000010001.00</p>"},{"location":"dataset_cards/omiga/","title":"OMIGA","text":"Dataset Cards - OMIGA Dataset Cards - OMIGA 2c_vs_64zg - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)Modified version of SMAC v1, popularised by MAPPO 2Discrete[478]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor8.91 \u00b1 1.012.5310.00108303481.00Medium13.00 \u00b1 1.3910.0115.003794010011.00Good19.94 \u00b1 1.2615.1821.615921510011.00</p>6h_vs_8z - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)Modified version of SMAC v1, popularised by MAPPO 6Discrete[172]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor9.12 \u00b1 0.814.809.992425510011.00Medium11.97 \u00b1 1.2610.0014.992951110011.00Good17.84 \u00b1 2.1515.0120.023804010011.00</p>5m_vs_6m - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)Modified version of SMAC v1, popularised by MAPPO 5Discrete[124]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor8.50 \u00b1 1.191.819.892274710010.96Medium11.03 \u00b1 0.5810.0811.962771710010.95Good20.00 \u00b1 0.0020.0020.002773410010.96</p>corridor - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeSMAC (v1)Modified version of SMAC v1, popularised by MAPPO 6Discrete[346]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoPoor4.93 \u00b1 1.710.009.995126810011.00Medium13.07 \u00b1 1.2710.0214.9912601210011.00Good19.88 \u00b1 1.0115.0120.4910017010011.00</p>6halfcheetah - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMAMuJoCoV1.0, Mujoco v2006Continuous[23]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMedium-Replay655.76 \u00b1 590.40-198.772132.60100100010001.00Medium-Expert2105.38 \u00b1 1073.24251.943866.09200200020001.00Medium1425.66 \u00b1 520.12251.942113.52100100010001.00Expert2785.10 \u00b1 1053.14317.943866.09100100010001.00</p>2ant - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMAMuJoCoV1.0, Mujoco v2002Continuous[113]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMedium-Replay1029.51 \u00b1 141.27895.371517.06175175017500.66Medium-Expert1736.88 \u00b1 319.64840.772124.15200200020001.00Medium1418.70 \u00b1 37.04840.771473.86100100010001.00Expert2055.07 \u00b1 22.071994.032124.15100100010001.00</p>3hopper - DownloadMetadata<p>Environment nameVersionAgentsAction typeObservation sizeReward typeMAMuJoCoV1.0, Mujoco v2003Continuous[14]Dense</p>Generation procedure for each dataset<p>Converted from omiga format to a Vault.</p>Summary statistics<p>UidEpisode return meanMin returnMax returnTransitionsTrajectoriesJoint SACoMedium-Replay746.42 \u00b1 671.8970.762801.15131482641601.00Medium-Expert1190.61 \u00b1 973.4095.273762.69191978254811.00Medium723.57 \u00b1 211.66128.382776.4991939140001.00Expert2452.02 \u00b1 1097.8695.273762.69100039114811.00</p>"},{"location":"research/","title":"Index","text":"<p>These pages detail the cutting-edge offline MARL research directions which utilise OG-MARL. We strive to update this list regularly. Please open a pull-request on the GitHub repo if you would like to be featured!</p>"},{"location":"research/oryx/","title":"Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL","text":""},{"location":"research/oryx/#overview","title":"Overview","text":"<p>A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works\u2014SMAC, RWARE, and Multi-Agent MuJoCo\u2014covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's surperior ability to scale effectively in such settings.</p> <p></p> <p> </p> <p>Performance of Oryx across diverse benchmark datasets from prior literature. Scores are normalised relative to the current state-of-the-art, with values above 1 indicating that Oryx surpasses previous best-known results. Unnormalized scores are provided in the appendix. Gold stars indicate instances where Oryx matches or exceeds state-of-the-art performance, while black stars denote otherwise. </p>"},{"location":"research/oryx/#new-datasets","title":"New Datasets","text":""},{"location":"research/oryx/#tmaze","title":"TMAZE","text":"<p>The T-Maze environment is intentionally designed as a minimalist setting that isolates key challenges for multi-agent reinforcement learning: interdependent action selection, reliance on memory from previous timesteps, and effective coordination to achieve a common goal</p> <p></p>"},{"location":"research/oryx/#connector","title":"Connector","text":"<p>We generated a range of datasets on Connector, a very challenging coorination task amongst agents. In Connector, agents must travel from their spawn location to a target location without cutting each other off. The team is rewarded in proportion to how many agents successfully made it to their goals. Datasets with up to 50 agents were generated. </p> <p></p> <p></p> Task Samples Mean Return Max Return Min Return con-5x5x3a 1.17\u202fM 0.59 0.97 \u22120.75 con-7x7x5a 1.13\u202fM 0.48 0.97 \u22121.23 con-10x10x10a 1.09\u202fM 0.40 0.97 \u22121.53 con-15x15x23a 1.06\u202fM 0.34 0.97 \u22121.56 con-18x18x30a 1.00\u202fM 0.25 0.97 \u22122.43 con-22x22x40a 624\u202f640 0.40 0.97 \u22122.61 con-25x25x50a 624\u202f640 0.33 0.97 \u22123.06"},{"location":"research/polygames/","title":"Coordination Failure in Cooperative Offline MARL","text":"<p>Paper | Notebook | Announcement</p> <p>What happens when trying to learn multi-agent coordination from a static dataset? Catastrophe, if you\u2019re not careful! This is the topic of our work on \u2728Coordination Failure in Offline Multi-Agent Reinforcement Learning \u2728</p> <p></p> <p>Many offline MARL methods build on an MADDPG-style update, which we call the \u201cBest Response Under Dataset\u201d (BRUD). Essentially, agents optimise their action in best response to the other agents\u2019 actions, as sampled from the dataset \ud83e\udd3c</p> <p> But this can lead to catastrophic miscoordination! \ud83e\udd4a </p> <p>To illustrate this phenomenon, we use polynomial games for tractable insights. For example, consider a simple game, \\(R = xy\\), dubbed the \"sign-agreement\" game. Agents X and Y aim to choose actions of the same sign (\\(++\\) or \\(--\\)) to yield good rewards. \ud83d\udcc8</p> <p></p> <p>Suppose in this game that Agent X currently takes a NEGATIVE action, and Agent Y currently takes a POSITIVE action\u2014illustrated by the Current Policy on the left. Now suppose we sample a point from the static dataset, where X took a POSITIVE action and Y took a negative action, illustrated on the right.</p> <p></p> <p>With a BRUD-style update, the agent policies will update according to the illustration below. Agent X looks at the datapoint, where Y took a negative action, and makes its action more negative in best response. The opposite happens for Agent Y when looking at the datapoint from X, making its action more positive.</p> <p></p> <p>The result is catastrophic! Agents move towards a low-reward region, in the opposite direction of the true optimal update. Our work goes further to ground this result mathematically, and demonstrates how and why other instances of miscoordination arise in a variety of polynomial games. \ud83e\udd13</p> <p></p> <p>How do we solve this problem? Our key insight is that miscoordination arises because of  the dissimilarity between the current joint policy output, and the sampled joint action.</p> <p> \u26a0\ufe0f Not all data is equally important at all times \u26a0\ufe0f </p> <p>Instead: we want to prioritise sampling experience from a dataset-generating policy similar to the current joint policy. We do this by setting the priorities to be inversely proportional to some function of the distance between the policies.</p> <p>We call this Proximal Joint-Action Prioritisation (PJAP) \ud83e\udd20</p> <p>Returning to the sign-agreement game from before, here we see how vanilla MADDPG using a static dataset fails to learn the optimal policy \ud83d\ude2d But the experience is just sampled uniformly from the dataset!</p> <p></p> <p>If we instead prioritise sampling actions that are close to our current joint policy, using PJAP, then MADDPG can find the optimal reward region! \ud83c\udf89</p> <p></p> <p>Here\u2019s a visualisation of the priorities in the underlying buffer. Prioritised experience replay is already a popular tool in RL, so PJAP can easily be integrated with existing code. \ud83d\ude0c</p> <p></p> <p>In a more complex polynomial game, clear improvement occurs once again. Crucially, we see how the mean distance between the sampled actions and current policy is reduced, which leads to higher returns. \ud83d\udc83</p> <p></p> <p>Excitingly, this result transfers to more complex scenarios! Here we look at 2halfcheetah from MAMuJoCo, and see that PJAP yields lower average distance between the sample actions and the current joint policy, which leads to statistically significant higher returns \ud83d\udc06\ud83d\udd25</p> <p></p> <p>Importantly, our work shows how insights drawn from simplified, tractable games can lead to useful, theoretically grounded insights that transfer to more complex contexts. A core dimension of offering is an interactive notebook, from which almost all of our results can be reproduced, simply in a browser! \ud83d\udcbb</p> <p> <p></p> </p> <p>We presented this paper at the ARLET workshop at ICML 2024.</p>"},{"location":"research/polygames/#cite","title":"Cite","text":"<pre><code>@inproceedings{tilbury2024coordination,\n    title={Coordination Failure in Cooperative Offline MARL},\n    author={Tilbury, Callum Rhys and Formanek, Juan Claude and Beyers, Louise and Shock, Jonathan Phillip and Pretorius, Arnu},\n    booktitle={ICML 2024 Workshop: Aligning Reinforcement Learning Experimentalists and Theorists},\n    year={2024},\n    url={https://arxiv.org/abs/2407.01343}\n}\n</code></pre>"},{"location":"research/selective-reincarnation/","title":"Selective Reincarnation in Multi-Agent Reinforcement Learning","text":"<p>Reincarnation in reinforcement learning has been proposed as a formalisation of reusing prior computation from past experiments when training an agent in an environment. In this work, we present a brief foray into the paradigm of reincarnation in the multi-agent reinforcement learning (MARL) context. We consider the case where only some agents are reincarnated, whereas the others are trained from scratch \u2014 selective reincarnation.</p>"},{"location":"research/selective-reincarnation/#selectively-reincarnated-policy-to-value-marl","title":"Selectively-Reincarnated Policy-to-Value MARL","text":"<p>In this work we present a case study in multi-agent policy-to-value RL (PVRL), focusing on one of the methods invoked by Agarwal et al. (2022), called \u2018Rehearsal\u2019 (G\u00fcl\u00e7ehre et al., 2020).</p> <p>For the sake of the current question of selective reincarnation, we use the 6-Agent HALFCHEETAH environment from Multi-Agent MuJoCo, where each of the six degrees-of-freedom is controlled by a separate agent.</p> <p></p> <p>We enumerate all combinations of agents for reincarnation, a total of 2^6 = 64 subsets. For each subset, we retrain the system on HALFCHEETAH, where that particular group of agents gains access to their teacher's offline data (i.e. they are reincarnated). For each combination, we train the system for 200k timesteps, remove the teacher data, and then train for a further 50k timesteps on student data alone.</p>"},{"location":"research/selective-reincarnation/#impact-of-teacher-dataset-quality-in-reincarnating-marl","title":"Impact of Teacher Dataset Quality in Reincarnating MARL","text":"<p>First, we show that fully reincarnating a MARL system can speed up convergence. Additionally, we show that providing access solely to Good teacher data initially does not help speed up training and even seems to hamper it. It is only after around 125k timesteps that we observe a dramatic peak in performance, thereafter significantly outperforming the tabula rasa system. In contrast, having additional Medium samples enables higher returns from the beginning of training \u2013 converging faster than the solely Good dataset.</p> <p></p>"},{"location":"research/selective-reincarnation/#arbitrarily-selective-reincarnation","title":"Arbitrarily Selective Reincarnation","text":"<p>Next we show that a selectively reincarnated setup also yields benefits \u2013 e.g. reincarnating with just half of the agents provides an improvement over tabula rasa.</p> <p></p>"},{"location":"research/selective-reincarnation/#targeted-selective-reincarnation-matters","title":"Targeted Selective Reincarnation Matters","text":"<p>Finally, we present a vital consideration: in a multi-agent system, even in the simpler homogeneous case, agents can sometimes assume dissimilar roles with different degrees of importance to the whole system. In the HALFCHEETAH environment particularly, consider the unique requirements for the ankle, knee, and hip joints, and how these differ across the front and back legs, in order for the cheetah to walk. It is thus important that we compare, for a given integer x, the results across various combinations of x reincarnated agents. That is, e.g., compare reincarnating the back ankle and back knee (BA, BK) with the back ankle and back hip (BA, BH). We find that the choice of which agents to reincarnate plays a significant role in the experiment\u2019s outcome.</p>"},{"location":"research/selective-reincarnation/#best-and-worst-of-three-reincarnated-agents","title":"Best and Worst of Three Reincarnated Agents","text":""},{"location":"research/selective-reincarnation/#best-and-worst-of-four-reincarnated-agents","title":"Best and Worst of Four Reincarnated Agents","text":""},{"location":"research/selective-reincarnation/#best-and-worst-of-five-reincarnated-agents","title":"Best and Worst of Five Reincarnated Agents","text":""},{"location":"research/selective-reincarnation/#cite","title":"Cite","text":"<pre><code>@inproceedings{\n    formanek2023selective,\n    title={Reduce, Reuse, Recycle: Selective Reincarnation in Multi-Agent Reinforcement Learning},\n    author={Juan Claude Formanek and Callum Rhys Tilbury and Jonathan Phillip Shock and Kale-ab Tessera and Arnu Pretorius},\n    booktitle={Workshop on Reincarnating Reinforcement Learning at ICLR 2023},\n    year={2023},\n    url={https://openreview.net/forum?id=_Nz9lt2qQfV}\n}\n</code></pre>"}]}